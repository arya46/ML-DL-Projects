{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Jigsaw 0 About Data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJcbo1rGhOVhqD+eV8O6nX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mZ6frQW41rO2","colab_type":"text"},"source":["# Jigsaw Unintended Bias in Toxicity Classification"]},{"cell_type":"markdown","metadata":{"id":"zpzky0MN1vGt","colab_type":"text"},"source":["At the end of 2017 the [Civil Comments](https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d) platform shut down and chose make their ~2m public comments from their platform available in a lasting open archive so that researchers could understand and improve civility in online conversations for years to come. Jigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IeWMNyDAFTwf","colab_type":"text"},"source":["# 1 About Data:\n","\n","In the data supplied for this competition, the text of the individual comment is found in the `comment_text` column. Each comment in Train has a toxicity label (`target`), and models should predict the `target` toxicity for the Test data.\n","\n","The data also has several additional toxicity subtype attributes. Models do not need to predict these attributes for the competition, they are included as an additional avenue for research. Subtype attributes are:\n","\n","- `severe_toxicity`\n","- `obscene`\n","- `threat`\n","- `insult`\n","- `identity_attack`\n","- `sexual_explicit`\n","\n","Additionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment.\n","\n","- `male`\n","- `female`\n","- `homosexual_gay_or_lesbian`\n","- `christian`\n","- `jewish`\n","- `muslim`\n","- `black`\n","- `white`\n","- `psychiatric_or_mental_illness`\n","\n","There is also some additional features:\n","\n","- `created_date`\n","- `publication_id`\n","- `parent_id`\n","- `article_id`\n","- `rating`\n","- `funny`\n","- `wow`\n","- `sad`\n","- `likes`\n","- `disagree`\n","- `sexual_explicit`\n","- `identity_annotator_count`\n","- `toxicity_annotator_count`"]},{"cell_type":"markdown","metadata":{"id":"7x7vd03lB_OS","colab_type":"text"},"source":["<h3> 1.1 Business Problem: </h3>\n","\n","The Conversation AI team (it is research initiated by Jigsaw and Google) build a toxicity model, they found that the model incorrectly learned to associate the names of frequently attacked identities with toxicity. So the model predicted high toxicity for those comments which contain words like gay, black, Muslim, white, lesbian, etc, even when comments were not actually toxic (e.g. I am a gay woman.). This happened because the dataset was collected from the sources where such words (or identities) are considered as highly offensive. A model is needed to be build which can find the __toxicity__ in the comments and minimize the __unintended bias__ with respect to some identities.\n","\n","- __Toxic__ comments are the comments which are offensive and sometimes can make some people leave the discussion (on public forums).\n","\n","- __Unintended Bias__ is related to unplanned bias which happened because the data was collected from such sources which considered some words (or identities) very offensive.\n"]},{"cell_type":"markdown","metadata":{"id":"jmX_3S3JJcTT","colab_type":"text"},"source":["<h3> 1.2 Objective: </h3>\n","\n","- Predicting whether a comment is toxic or not.\n","- Minimize unintended bias."]},{"cell_type":"markdown","metadata":{"id":"jTjMXFsuJ00R","colab_type":"text"},"source":["<h3> 1.3 Constraints: </h3>\n","\n","- No strict latency requirements."]},{"cell_type":"markdown","metadata":{"id":"etVDwEiJJ-aP","colab_type":"text"},"source":["# 2 Mapping the real-world problem to a ML problem:"]},{"cell_type":"markdown","metadata":{"id":"sLLODIM9KEJo","colab_type":"text"},"source":["<h3> 2.1 Type of Machine Learning Problem </h3> \n","\n","This is a binary classification task: \n","\n","- Target label 0 means __non-toxic__ comments \n","\n","- Target label 1 means __toxic__ comments."]},{"cell_type":"markdown","metadata":{"id":"YrsgXAvoKEo_","colab_type":"text"},"source":["<h3> 2.2 Performance Metric </h3> \n","\n","- Source: https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation\n","\n","- Metric(s):\n","\n","This competition uses a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.\n","\n","First, we'll define each submetric.\n","\n","__Overall AUC:__\n","This is the ROC-AUC for the full evaluation set.\n","\n","__Bias AUCs:__\n","To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias. More about these metrics in Conversation AI's recent paper [Nuanced Metrics for Measuring Unintended Bias with Real Data in Text Classification](https://arxiv.org/abs/1903.04561).\n","\n","*Subgroup AUC:* Here, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n","\n","*BPSN (Background Positive, Subgroup Negative) AUC:* Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n","\n","*BNSP (Background Negative, Subgroup Positive) AUC:* Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n","\n","__Generalized Mean of Bias AUCs:__\n","\n","To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n","\n","![Generalized Mean of Bias AUCs](https://miro.medium.com/max/288/1*mdaEgvW3QN3nD1HjRDSgoQ.png)\n","\n","![Variables](https://miro.medium.com/max/474/1*OIxmlRN66YE23g8kvDckAQ.png)\n","\n","\n","\n","__Final Metric/Score/AUC:__\n","\n","![final auc](https://miro.medium.com/max/560/1*oWQoDSnOt41GTWDp8V9FbA.png)\n","\n","\n","![variable](https://miro.medium.com/max/1043/1*Ufhmj7YkqXooBHTtG_16cw.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B0jmUv9MKL0Q","colab_type":"text"},"source":["<h3> 2.3. Machine Learing Objectives and Constraints </h3> \n","\n","- Objective: Predict the probability of each data-point whether it is toxic or non-toxic.\n","Also, maximize the final score defined in above section.\n","\n","- Constraints: There is no strict latency requirements. However, we want to penalize the data-points that mentions identity."]}]}